{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout, Input,InputLayer, Activation, BatchNormalization\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, GlobalAveragePooling2D, ZeroPadding2D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['NORMAL', 'PNEUMONIA']\n",
    "img_size = 224\n",
    "\n",
    "def get_data(data_dir):\n",
    "    data = []\n",
    "    for label in labels:\n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_path = os.path.join(path, img)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    continue  # Skip if the image cannot be loaded\n",
    "\n",
    "                resized_arr = cv2.resize(img, (img_size, img_size))\n",
    "                \n",
    "                contrast = int((177 - 0) * (127 - (-127)) / (254 - 0) + (-127))\n",
    "                Alpha = float(131 * (contrast + 127)) / (127 * (131 - contrast))\n",
    "                Gamma = 127 * (1 - Alpha)\n",
    "                cntrst = cv2.addWeighted(resized_arr, Alpha, resized_arr, 0, Gamma)\n",
    "                \n",
    "                data.append([cntrst, class_num])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_path}: {e}\")\n",
    "    return np.array(data, dtype=object)\n",
    "\n",
    "train = get_data(r\"D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\train\")\n",
    "test = get_data(r\"D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\test\")\n",
    "val = get_data(r\"D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_val=[]\n",
    "y_val=[]\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for feature, label in train:\n",
    "  x_train.append(feature)\n",
    "  y_train.append(label)\n",
    "\n",
    "for feature, label in test:\n",
    "  x_test.append(feature)\n",
    "  y_test.append(label)\n",
    "\n",
    "for feature, label in val:\n",
    "  x_val.append(feature)\n",
    "  y_val.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = np.array(x_train,dtype=np.float16) / 255\n",
    "x_test = np.array(x_test,dtype=np.float16) / 255\n",
    "x_val = np.array(x_val,dtype=np.float16) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.reshape(-1, img_size, img_size, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test.reshape(-1, img_size, img_size, 1)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "x_val.reshape(-1, img_size, img_size, 1)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved models\n",
    "resnet50_model = tf.keras.models.load_model(r'D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\final upload drive\\restnet50saved.h5')\n",
    "inception_model = tf.keras.models.load_model(r'D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\final upload drive\\inceptionv3saved.h5')\n",
    "vgg19_model = tf.keras.models.load_model(r'D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\final upload drive\\vgg19saved.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate and print metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3s/step\n",
      "VGG19 Model Metrics:\n",
      "Accuracy: 0.9444444444444444\n",
      "Precision: 0.9\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9473684210526315\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5s/step\n",
      "\n",
      "ResNet50 Model Metrics:\n",
      "Accuracy: 0.9722222222222222\n",
      "Precision: 0.9473684210526315\n",
      "Recall: 1.0\n",
      "F1 Score: 0.972972972972973\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018B45CD5BD0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018B45CD5BD0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 7s/stepWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018B45CD5BD0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018B45CD5BD0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step\n",
      "\n",
      "InceptionV3 Model Metrics:\n",
      "Accuracy: 0.8611111111111112\n",
      "Precision: 0.782608695652174\n",
      "Recall: 1.0\n",
      "F1 Score: 0.8780487804878049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8611111111111112, 0.782608695652174, 1.0, 0.8780487804878049)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19_predictions = vgg19_model.predict(x_val)\n",
    "vgg19_predicted_classes = (vgg19_predictions > 0.5).astype(int).flatten()\n",
    "print(\"VGG19 Model Metrics:\")\n",
    "calculate_metrics(y_val.flatten(), vgg19_predicted_classes)\n",
    "\n",
    "# Predict and calculate metrics for ResNet50 model\n",
    "resnet50_predictions = resnet50_model.predict(x_val)\n",
    "resnet50_predicted_classes = (resnet50_predictions > 0.5).astype(int).flatten()\n",
    "print(\"\\nResNet50 Model Metrics:\")\n",
    "calculate_metrics(y_val.flatten(), resnet50_predicted_classes)\n",
    "\n",
    "# Predict and calculate metrics for InceptionV3 model\n",
    "inceptionv3_predictions = inception_model.predict(x_val)\n",
    "inceptionv3_predicted_classes = (inceptionv3_predictions > 0.5).astype(int).flatten()\n",
    "print(\"\\nInceptionV3 Model Metrics:\")\n",
    "calculate_metrics(y_val.flatten(), inceptionv3_predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION REPORT OF VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 13s/step\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   NORMAL (Class 0)       0.98      0.44      0.61       234\n",
      "PNEUMONIA (Class 1)       0.75      0.99      0.85       390\n",
      "\n",
      "           accuracy                           0.79       624\n",
      "          macro avg       0.87      0.72      0.73       624\n",
      "       weighted avg       0.84      0.79      0.76       624\n",
      "\n",
      "Accuracy: 0.7885\n",
      "Precision: 0.7490\n",
      "Recall: 0.9949\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming you have your test data (x_test and y_test)\n",
    "# Make predictions for the test set\n",
    "predictions = vgg19_model.predict(x_test)\n",
    "\n",
    "# Convert the predicted probabilities to class labels for binary classification\n",
    "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# Ensure y_test is in binary labels format (0 or 1)\n",
    "y_test_classes = y_test.flatten()  # Flatten if necessary, assuming y_test is already shaped appropriately\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_classes, predicted_classes, target_names=['NORMAL (Class 0)', 'PNEUMONIA (Class 1)']))\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(y_test_classes, predicted_classes)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate and print the precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_test_classes, predicted_classes, zero_division=1)\n",
    "recall = recall_score(y_test_classes, predicted_classes, zero_division=1)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION REPORT OF INCEPTIONV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3s/step\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   NORMAL (Class 0)       0.98      0.48      0.65       234\n",
      "PNEUMONIA (Class 1)       0.76      0.99      0.86       390\n",
      "\n",
      "           accuracy                           0.80       624\n",
      "          macro avg       0.87      0.74      0.76       624\n",
      "       weighted avg       0.84      0.80      0.78       624\n",
      "\n",
      "Accuracy: 0.8029\n",
      "Precision: 0.7623\n",
      "Recall: 0.9949\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming you have your test data (x_test and y_test)\n",
    "# Make predictions for the test set\n",
    "predictions = inception_model.predict(x_test)\n",
    "\n",
    "# Convert the predicted probabilities to class labels for binary classification\n",
    "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# Ensure y_test is in binary labels format (0 or 1)\n",
    "y_test_classes = y_test.flatten()  # Flatten if necessary, assuming y_test is already shaped appropriately\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_classes, predicted_classes, target_names=['NORMAL (Class 0)', 'PNEUMONIA (Class 1)']))\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(y_test_classes, predicted_classes)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate and print the precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_test_classes, predicted_classes, zero_division=1)\n",
    "recall = recall_score(y_test_classes, predicted_classes, zero_division=1)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION REPORT OF RESTNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   NORMAL (Class 0)       1.00      0.43      0.60       234\n",
      "PNEUMONIA (Class 1)       0.75      1.00      0.85       390\n",
      "\n",
      "           accuracy                           0.79       624\n",
      "          macro avg       0.87      0.72      0.73       624\n",
      "       weighted avg       0.84      0.79      0.76       624\n",
      "\n",
      "Accuracy: 0.7869\n",
      "Precision: 0.7457\n",
      "Recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming you have your test data (x_test and y_test)\n",
    "# Make predictions for the test set\n",
    "predictions = resnet50_model.predict(x_test)\n",
    "\n",
    "# Convert the predicted probabilities to class labels for binary classification\n",
    "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# Ensure y_test is in binary labels format (0 or 1)\n",
    "y_test_classes = y_test.flatten()  # Flatten if necessary, assuming y_test is already shaped appropriately\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_classes, predicted_classes, target_names=['NORMAL (Class 0)', 'PNEUMONIA (Class 1)']))\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(y_test_classes, predicted_classes)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate and print the precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_test_classes, predicted_classes, zero_division=1)\n",
    "recall = recall_score(y_test_classes, predicted_classes, zero_division=1)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to load and evaluate a model\n",
    "def evaluate_model(model_path, x_val, y_val):\n",
    "    # Load the saved model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # Make predictions for the validation set\n",
    "    predictions = model.predict(x_val)\n",
    "    \n",
    "    # Convert the predicted probabilities to class labels for binary classification\n",
    "    predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "    # Ensure y_val is in binary labels format (0 or 1)\n",
    "    y_val_classes = y_val.flatten()\n",
    "\n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = accuracy_score(y_val_classes, predicted_classes)\n",
    "    precision = precision_score(y_val_classes, predicted_classes, zero_division=1)\n",
    "    recall = recall_score(y_val_classes, predicted_classes, zero_division=1)\n",
    "    f1 = f1_score(y_val_classes, predicted_classes, zero_division=1)\n",
    "    \n",
    "    # Return the calculated metrics\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "# List of model paths\n",
    "model_paths = [\n",
    "    r'D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\final upload drive\\restnet50saved.h5',\n",
    "    r'D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\final upload drive\\inceptionv3saved.h5',\n",
    "    r'D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\final upload drive\\vgg19saved.h5'\n",
    "]\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Evaluate each model and store the results\n",
    "for model_path in model_paths:\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model_path, x_val, y_val)\n",
    "    results[model_path] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\final upload drive\\restnet50saved.h5\n",
      "Accuracy: 0.9722\n",
      "Precision: 0.9474\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9730\n",
      "\n",
      "\n",
      "Model: D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\final upload drive\\inceptionv3saved.h5\n",
      "Accuracy: 0.8611\n",
      "Precision: 0.7826\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.8780\n",
      "\n",
      "\n",
      "Model: D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\final upload drive\\vgg19saved.h5\n",
      "Accuracy: 0.9444\n",
      "Precision: 0.9000\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9474\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the results for each model\n",
    "for model_path, metrics in results.items():\n",
    "    print(f\"Model: {model_path}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: D:\\purpose.D drive\\pneumoniaxray\\xray_chest\\final upload drive\\restnet50saved.h5\n",
      "Best Model Accuracy: 0.9722\n",
      "Best Model Precision: 0.9474\n",
      "Best Model Recall: 1.0000\n",
      "Best Model F1 Score: 0.9730\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the best model based on accuracy\n",
    "best_model = max(results, key=lambda x: results[x]['accuracy'])\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Best Model Accuracy: {results[best_model]['accuracy']:.4f}\")\n",
    "print(f\"Best Model Precision: {results[best_model]['precision']:.4f}\")\n",
    "print(f\"Best Model Recall: {results[best_model]['recall']:.4f}\")\n",
    "print(f\"Best Model F1 Score: {results[best_model]['f1_score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envxray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
